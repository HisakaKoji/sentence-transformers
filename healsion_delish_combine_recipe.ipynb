{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled79.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPRRSuc7S0CM99u5jG2PaYm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HisakaKoji/sentence-transformers/blob/master/healsion_delish_combine_recipe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eTLvwzf6pmz",
        "colab_type": "code",
        "outputId": "fd8447e1-dbaf-4a60-d833-4e83b7aafb91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/sonoisa/sentence-transformers\n",
        "!cd sentence-transformers; pip install -r requirements.txt\n",
        "!wget -O sonobe-datasets-sentence-transformers-model.tar \"https://www.floydhub.com/api/v1/resources/JLTtbaaK5dprnxoJtUbBbi?content=true&download=true&rename=sonobe-datasets-sentence-transformers-model-2\"\n",
        "!tar -xvf sonobe-datasets-sentence-transformers-model.tar"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'sentence-transformers'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 707 (delta 2), reused 6 (delta 2), pack-reused 701\u001b[K\n",
            "Receiving objects: 100% (707/707), 195.58 KiB | 296.00 KiB/s, done.\n",
            "Resolving deltas: 100% (475/475), done.\n",
            "Collecting transformers==2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 34.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (4.38.0)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (1.18.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (3.2.5)\n",
            "Collecting mecab-python3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/49/b55a839a77189042960bf96490640c44816073f917d489acbc5d79fa5cc3/mecab_python3-0.996.5-cp36-cp36m-manylinux2010_x86_64.whl (17.1MB)\n",
            "\u001b[K     |████████████████████████████████| 17.1MB 202kB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->-r requirements.txt (line 1)) (1.12.23)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 50.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->-r requirements.txt (line 1)) (2.21.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 46.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->-r requirements.txt (line 1)) (2019.12.20)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->-r requirements.txt (line 7)) (1.12.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->-r requirements.txt (line 1)) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.23 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->-r requirements.txt (line 1)) (1.15.23)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->-r requirements.txt (line 1)) (0.9.5)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->-r requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->-r requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->-r requirements.txt (line 1)) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->-r requirements.txt (line 1)) (2.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.3.0->-r requirements.txt (line 1)) (7.1.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.23->boto3->transformers==2.3.0->-r requirements.txt (line 1)) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.23->boto3->transformers==2.3.0->-r requirements.txt (line 1)) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=f4fb02b42119ac71ec518a140dcac9f0c3edee4f6293c2fe0173c5f8b4192675\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, transformers, mecab-python3\n",
            "Successfully installed mecab-python3-0.996.5 sacremoses-0.0.38 sentencepiece-0.1.85 transformers-2.3.0\n",
            "--2020-03-24 03:56:19--  https://www.floydhub.com/api/v1/resources/JLTtbaaK5dprnxoJtUbBbi?content=true&download=true&rename=sonobe-datasets-sentence-transformers-model-2\n",
            "Resolving www.floydhub.com (www.floydhub.com)... 104.26.1.30, 104.26.0.30, 2606:4700:20::681a:11e, ...\n",
            "Connecting to www.floydhub.com (www.floydhub.com)|104.26.1.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/tar]\n",
            "Saving to: ‘sonobe-datasets-sentence-transformers-model.tar’\n",
            "\n",
            "sonobe-datasets-sen     [               <=>  ] 422.28M  9.56MB/s    in 46s     \n",
            "\n",
            "2020-03-24 03:57:05 (9.23 MB/s) - ‘sonobe-datasets-sentence-transformers-model.tar’ saved [442788352]\n",
            "\n",
            "./\n",
            "./training_bert_japanese/\n",
            "./training_bert_japanese/0_BERTJapanese/\n",
            "./training_bert_japanese/0_BERTJapanese/added_tokens.json\n",
            "./training_bert_japanese/0_BERTJapanese/config.json\n",
            "./training_bert_japanese/0_BERTJapanese/pytorch_model.bin\n",
            "./training_bert_japanese/0_BERTJapanese/sentence_bert_config.json\n",
            "./training_bert_japanese/0_BERTJapanese/special_tokens_map.json\n",
            "./training_bert_japanese/0_BERTJapanese/tokenizer_config.json\n",
            "./training_bert_japanese/0_BERTJapanese/vocab.txt\n",
            "./training_bert_japanese/1_Pooling/\n",
            "./training_bert_japanese/1_Pooling/config.json\n",
            "./training_bert_japanese/config.json\n",
            "./training_bert_japanese/modules.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LdiGS3_7FfC",
        "colab_type": "code",
        "outputId": "9786c23e-f144-447c-9a32-5fa4e45dbf66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%cd sentence-transformers"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'sentence-transformers'\n",
            "/content/sentence-transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzwaFQcF7HCf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "model_path = \"/content/training_bert_japanese\"\n",
        "model = SentenceTransformer(model_path, show_progress_bar=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcSVgPsh7K9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh8nAv7EHoWW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('all_delish.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UeIdztzIkP-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChZolfcRHyIb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['description'] = df['店舗名'] + ' ' + df['description'] # + ' ' + df['step']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjlDN33aJc22",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ld0LAJ3m8JOH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.rename(columns={'店舗名': 'name'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEPQiNkgMMFY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.dropna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq_4qM0llVbG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df[~df.duplicated(subset='url')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dqyike2XloZg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfrCgcps7br3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = df['description'].values.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUmCZ-Gx7g2X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_vectors = model.encode(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF9r1fhlN7Fr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "70d44f1d-4111-4ca5-a869-c84b47be403c"
      },
      "source": [
        "sentence_vectors[0][0]"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.28582418"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoR3Q9viR8Mf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nySOxCfWRv-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data1 = np.empty(768)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1l1HZYpLRy8a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data1 = sentence_vectors[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyM3LHJkSFks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9PAAFbORmNV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data1 = np.vstack((data1, sentence_vectors[2] ) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyhY3-AwNECd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_vector = pd.Series(sentence_vectors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_H4NqXOODKS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6a9de30c-96b6-4e2b-9eec-2478dd0a1002"
      },
      "source": [
        "df_vector[0].shape"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(768,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLtBftAN8vV7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences_name = df['name'].values.tolist()\n",
        "sentences_url = df['url'].values.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwfKoVBh7yYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "num_clusters = 8\n",
        "clustering_model = KMeans(n_clusters=num_clusters)\n",
        "clustering_model.fit(sentence_vectors)\n",
        "cluster_assignment = clustering_model.labels_\n",
        "\n",
        "clustered_sentences = [[] for i in range(num_clusters)]\n",
        "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
        "    clustered_sentences[cluster_id].append(sentences_name[sentence_id][:10])\n",
        "\n",
        "for i, cluster in enumerate(clustered_sentences):\n",
        "    print(\"Cluster \", i+1)\n",
        "    print(cluster)\n",
        "    print(\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P333vaBd6Kzf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_queries = pd.read_csv('cookpad_keyword.txt',header = None)\n",
        "#df_queries = pd.read_csv('healsio_recipe.csv',header = None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkRXb-B41ycQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('healsio_recipe.csv')\n",
        "df['description'] = df['name'] # + df['ingredient'] + df['step']\n",
        "queries =  df['description'].values.tolist()\n",
        "sentences_names = df['name'].values.tolist()\n",
        "sentences_urls = df['url'].values.tolist()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIJoqeDa8KLK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data1 = np.ndarray([])\n",
        "import scipy.spatial\n",
        "\n",
        "#queries = ['ゆきぽよ','美術館', 'グルメ', '究極の料理','スポーツ大会', '子供がはしゃげる','東広島市　料理','東広島市　おでかけ','古墳','アニメ','細菌','ビッグバン','甲賀忍者']\n",
        "#queries = df_queries[0].values.tolist()\n",
        "query_embeddings = model.encode(queries)\n",
        "\n",
        "closest_n = 5\n",
        "\n",
        "for i,(name,url,query, query_embedding) in enumerate(zip(sentences_names,sentences_urls,queries, query_embeddings)):\n",
        "    distances = scipy.spatial.distance.cdist([query_embedding], sentence_vectors, metric=\"cosine\")[0]\n",
        "\n",
        "    results = zip(range(len(distances)), distances)\n",
        "    results = sorted(results, key=lambda x: x[1])\n",
        "\n",
        "    print(\"\\n\\n======================\\n\\n\")\n",
        "    print(\"Query:\", name + ' ' + url)\n",
        "    print(\"\\nTop 5 most similar sentences in corpus:\")\n",
        "\n",
        "    for j,(idx, distance) in enumerate(results[0:closest_n]):\n",
        "        print(sentences_name[idx].strip(), sentences_url[idx].strip(),\"(Score: %.4f)\" % (distance / 2),idx)\n",
        "        if j == 0 and i == 0:\n",
        "          data1 = sentence_vectors[idx]\n",
        "          print(idx)\n",
        "        elif j == 0:\n",
        "          data1 = np.vstack((data1, sentence_vectors[idx] ) )\n",
        "          print(idx)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJltV8Z_skU2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_h = pd.read_csv('healsio_recipe.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nntWdupxtrym",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "df_h['description'] = df_h['name'] #  +  df['step']#+ df['ingredient'] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXmaG4G3uF2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_h = df_h[~df_h.duplicated(subset='url')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cT5CrdvtuI_o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences =  df_h['description'].values.tolist()\n",
        "sentences_name = df_h['name'].values.tolist()\n",
        "sentences_url = df_h['url'].values.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PD7D-hertQnX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_vectors = model.encode(sentences)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnMWbiBztSYe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df_queries = pd.read_csv('cookpad_keyword.txt',header = None)\n",
        "df_queries = pd.read_csv('kurashiru_keyword.txt',header = None)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ark-rDSVwvYF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_out = pd.read_csv('output.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvxEV-HntThd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy.spatial\n",
        "\n",
        "queries = ['ゆきぽよ','美術館', 'グルメ', '究極の料理','スポーツ大会', '子供がはしゃげる','東広島市　料理','東広島市　おでかけ','古墳','アニメ','細菌','ビッグバン','甲賀忍者']\n",
        "queries = df_queries[0].values.tolist()\n",
        "query_embeddings = model.encode(queries)\n",
        "\n",
        "closest_n = 5\n",
        "adata = pd.DataFrame()\n",
        "for query, query_embedding in zip(queries, query_embeddings):\n",
        "    distances = scipy.spatial.distance.cdist([query_embedding], data1, metric=\"cosine\")[0]\n",
        "\n",
        "    results = zip(range(len(distances)), distances)\n",
        "    results = sorted(results, key=lambda x: x[1])\n",
        "\n",
        "    print(\"\\n\\n======================\\n\\n\")\n",
        "    print(\"Query:\", query)\n",
        "    print(\"\\nTop 5 most similar sentences in corpus:\")\n",
        "    i = 0\n",
        "    for idx, distance in results[0:closest_n]:\n",
        "        i+=1\n",
        "        print(sentences_name[idx].strip(), sentences_url[idx].strip(),\"(Score: %.4f)\" % (distance / 2))\n",
        "        temp = pd.DataFrame({'query': query,\n",
        "                   'name':sentences_name[idx].strip(),'rank': i,\n",
        "                   'url': sentences_url})\n",
        "        adata = pd.concat([adata,temp], axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VG5Ty2PA_TZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "import os\n",
        "logs_base_dir = \"runs\"\n",
        "os.makedirs(logs_base_dir, exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbPTJ-nnBA3A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorboard as tb\n",
        "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n",
        "\n",
        "summary_writer = SummaryWriter()\n",
        "summary_writer.add_embedding(mat=np.array(sentence_vectors), metadata=sentences_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydYLaWoKBCOI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir {logs_base_dir}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhgZhlUoGI1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kill 1265"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}