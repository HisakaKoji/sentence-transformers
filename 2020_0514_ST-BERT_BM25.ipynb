{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled79.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOmsEsSSmGDzv+elE8ckUWA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HisakaKoji/sentence-transformers/blob/master/2020_0514_ST-BERT_BM25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eTLvwzf6pmz",
        "colab_type": "code",
        "outputId": "4dee2a55-7cd7-4f3c-aaac-ea16d1953726",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/sonoisa/sentence-transformers\n",
        "!cd sentence-transformers; pip install -r requirements.txt\n",
        "!wget -O sonobe-datasets-sentence-transformers-model.tar \"https://www.floydhub.com/api/v1/resources/JLTtbaaK5dprnxoJtUbBbi?content=true&download=true&rename=sonobe-datasets-sentence-transformers-model-2\"\n",
        "!tar -xvf sonobe-datasets-sentence-transformers-model.tar"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'sentence-transformers'...\n",
            "remote: Enumerating objects: 707, done.\u001b[K\n",
            "remote: Total 707 (delta 0), reused 0 (delta 0), pack-reused 707\u001b[K\n",
            "Receiving objects: 100% (707/707), 195.39 KiB | 310.00 KiB/s, done.\n",
            "Resolving deltas: 100% (476/476), done.\n",
            "Collecting transformers==2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 11.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (1.5.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (1.18.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (3.2.5)\n",
            "Collecting mecab-python3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/49/b55a839a77189042960bf96490640c44816073f917d489acbc5d79fa5cc3/mecab_python3-0.996.5-cp36-cp36m-manylinux2010_x86_64.whl (17.1MB)\n",
            "\u001b[K     |████████████████████████████████| 17.1MB 1.4MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/88/49e772d686088e1278766ad68a463513642a2a877487decbd691dec02955/sentencepiece-0.1.90-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 52.4MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 50.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->-r requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->-r requirements.txt (line 1)) (1.13.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->-r requirements.txt (line 1)) (2019.12.20)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.1->-r requirements.txt (line 3)) (0.16.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->-r requirements.txt (line 7)) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.3.0->-r requirements.txt (line 1)) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->-r requirements.txt (line 1)) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->-r requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->-r requirements.txt (line 1)) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->-r requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->-r requirements.txt (line 1)) (1.16.4)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->-r requirements.txt (line 1)) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->-r requirements.txt (line 1)) (0.3.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.4->boto3->transformers==2.3.0->-r requirements.txt (line 1)) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.4->boto3->transformers==2.3.0->-r requirements.txt (line 1)) (2.8.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=697a0f1f8a11a630bd0af879c6d8fc9d48b84ce1c1d5299e3fc7d806c5954a12\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, transformers, mecab-python3\n",
            "Successfully installed mecab-python3-0.996.5 sacremoses-0.0.43 sentencepiece-0.1.90 transformers-2.3.0\n",
            "--2020-05-14 05:21:41--  https://www.floydhub.com/api/v1/resources/JLTtbaaK5dprnxoJtUbBbi?content=true&download=true&rename=sonobe-datasets-sentence-transformers-model-2\n",
            "Resolving www.floydhub.com (www.floydhub.com)... 104.26.1.30, 104.26.0.30, 2606:4700:20::681a:11e, ...\n",
            "Connecting to www.floydhub.com (www.floydhub.com)|104.26.1.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/tar]\n",
            "Saving to: ‘sonobe-datasets-sentence-transformers-model.tar’\n",
            "\n",
            "sonobe-datasets-sen     [        <=>         ] 422.28M  10.9MB/s    in 14m 54s \n",
            "\n",
            "2020-05-14 05:36:36 (484 KB/s) - ‘sonobe-datasets-sentence-transformers-model.tar’ saved [442788352]\n",
            "\n",
            "./\n",
            "./training_bert_japanese/\n",
            "./training_bert_japanese/0_BERTJapanese/\n",
            "./training_bert_japanese/0_BERTJapanese/added_tokens.json\n",
            "./training_bert_japanese/0_BERTJapanese/config.json\n",
            "./training_bert_japanese/0_BERTJapanese/pytorch_model.bin\n",
            "./training_bert_japanese/0_BERTJapanese/sentence_bert_config.json\n",
            "./training_bert_japanese/0_BERTJapanese/special_tokens_map.json\n",
            "./training_bert_japanese/0_BERTJapanese/tokenizer_config.json\n",
            "./training_bert_japanese/0_BERTJapanese/vocab.txt\n",
            "./training_bert_japanese/1_Pooling/\n",
            "./training_bert_japanese/1_Pooling/config.json\n",
            "./training_bert_japanese/config.json\n",
            "./training_bert_japanese/modules.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrrjW0XqC2Ff",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "outputId": "5788008a-82b3-4311-a3c0-53ee847c8198"
      },
      "source": [
        "pip install git+https://github.com/boudinfl/pke.git"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/boudinfl/pke.git\n",
            "  Cloning https://github.com/boudinfl/pke.git to /tmp/pip-req-build-0bkanngp\n",
            "  Running command git clone -q https://github.com/boudinfl/pke.git /tmp/pip-req-build-0bkanngp\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from pke==1.8.1) (3.2.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from pke==1.8.1) (2.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pke==1.8.1) (1.18.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pke==1.8.1) (1.4.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (from pke==1.8.1) (2.2.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from pke==1.8.1) (1.12.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from pke==1.8.1) (0.0)\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 14.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pke==1.8.1) (0.16.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from pke==1.8.1) (0.14.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->pke==1.8.1) (4.4.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy->pke==1.8.1) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy->pke==1.8.1) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy->pke==1.8.1) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->pke==1.8.1) (0.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy->pke==1.8.1) (46.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->pke==1.8.1) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy->pke==1.8.1) (4.41.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->pke==1.8.1) (2.0.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy->pke==1.8.1) (1.0.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->pke==1.8.1) (3.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->pke==1.8.1) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->pke==1.8.1) (1.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->pke==1.8.1) (0.22.2.post1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->pke==1.8.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->pke==1.8.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->pke==1.8.1) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->pke==1.8.1) (2020.4.5.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->pke==1.8.1) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->pke==1.8.1) (3.1.0)\n",
            "Building wheels for collected packages: pke\n",
            "  Building wheel for pke (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pke: filename=pke-1.8.1-cp36-none-any.whl size=8759715 sha256=68be473fd88f14b23cda5e35338232fa4b41778619b9b93680f57189ccfcbe82\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8kwigazo/wheels/8d/24/54/6582e854e9e32dd6c632af6762b3a5d2f6b181c2992e165462\n",
            "Successfully built pke\n",
            "Installing collected packages: unidecode, pke\n",
            "Successfully installed pke-1.8.1 unidecode-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlwOPCML67my",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "4468ae75-4a77-4b47-8bed-f9e6c045e2cf"
      },
      "source": [
        "!python -m nltk.downloader stopwords"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDDuZDAk6_Ck",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "811c3dc5-1d13-401d-9ce3-cdbda37104ad"
      },
      "source": [
        "pip install \"https://github.com/megagonlabs/ginza/releases/download/latest/ginza-latest.tar.gz\""
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting https://github.com/megagonlabs/ginza/releases/download/latest/ginza-latest.tar.gz\n",
            "  Downloading https://github.com/megagonlabs/ginza/releases/download/latest/ginza-latest.tar.gz\n",
            "Requirement already satisfied (use --upgrade to upgrade): ginza==2.2.1 from https://github.com/megagonlabs/ginza/releases/download/latest/ginza-latest.tar.gz in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: spacy>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from ginza==2.2.1) (2.2.4)\n",
            "Requirement already satisfied: ja_ginza@ https://github.com/megagonlabs/ginza/releases/download/v2.2.0/ja_ginza-2.2.0.tar.gz from https://github.com/megagonlabs/ginza/releases/download/v2.2.0/ja_ginza-2.2.0.tar.gz in /usr/local/lib/python3.6/dist-packages (from ginza==2.2.1) (2.2.0)\n",
            "Requirement already satisfied: SudachiPy>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from ginza==2.2.1) (0.4.4)\n",
            "Requirement already satisfied: SudachiDict_core@ https://github.com/megagonlabs/ginza/releases/download/v2.2.0/SudachiDict_core-20190927.tar.gz from https://github.com/megagonlabs/ginza/releases/download/v2.2.0/SudachiDict_core-20190927.tar.gz in /usr/local/lib/python3.6/dist-packages (from ginza==2.2.1) (20190927)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.0->ginza==2.2.1) (1.18.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.0->ginza==2.2.1) (2.0.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.0->ginza==2.2.1) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.0->ginza==2.2.1) (4.41.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.0->ginza==2.2.1) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.0->ginza==2.2.1) (46.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.0->ginza==2.2.1) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.0->ginza==2.2.1) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.0->ginza==2.2.1) (3.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.0->ginza==2.2.1) (0.6.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.0->ginza==2.2.1) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.0->ginza==2.2.1) (1.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.0->ginza==2.2.1) (1.0.2)\n",
            "Requirement already satisfied: dartsclone~=0.9.0 in /usr/local/lib/python3.6/dist-packages (from SudachiPy>=0.4.0->ginza==2.2.1) (0.9.0)\n",
            "Requirement already satisfied: sortedcontainers~=2.1.0 in /usr/local/lib/python3.6/dist-packages (from SudachiPy>=0.4.0->ginza==2.2.1) (2.1.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->ginza==2.2.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->ginza==2.2.1) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->ginza==2.2.1) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->ginza==2.2.1) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.0->ginza==2.2.1) (1.6.0)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from dartsclone~=0.9.0->SudachiPy>=0.4.0->ginza==2.2.1) (0.29.17)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.0->ginza==2.2.1) (3.1.0)\n",
            "Building wheels for collected packages: ginza\n",
            "  Building wheel for ginza (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ginza: filename=ginza-2.2.1-cp36-none-any.whl size=11780 sha256=c985fbd9d63cb2e1aa552b73cab3f7cea69843e41cbf05fface50bfe314ee22b\n",
            "  Stored in directory: /root/.cache/pip/wheels/6f/97/43/f9c8c315bcd598f595e96aae207b120a3b84564f03ddd8d6a7\n",
            "Successfully built ginza\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41EZDAOT7Cgu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "f6590496-ced2-472d-dfc2-76c4315173b3"
      },
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (46.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icXM8XzM7H2n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pke\n",
        "pke.base.ISO_to_language['ja_ginza'] = 'japanese'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILOk6qHz7KGv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import ginza\n",
        "import nltk\n",
        "stopwords = list(ginza.STOP_WORDS)\n",
        "nltk.corpus.stopwords.words_org = nltk.corpus.stopwords.words\n",
        "nltk.corpus.stopwords.words = lambda lang : stopwords if lang == 'japanese' else nltk.corpus.stopwords.words_org(lang)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFYv9DHl96lb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('ja_ginza')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRNZ5LlM997-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# 引用元：「東京ディズニーランド」『フリー百科事典　ウィキペディア日本語版』。\n",
        "# 最終更新 2019年9月29日 (日) 04:02 UTC、URL: https://ja.wikipedia.org\n",
        "text = \"東京ディズニーランド、英称：Tokyo Disneyland、略称：TDL）は、\" +\\\n",
        "\"千葉県浦安市舞浜にあるディズニーリゾートを形成する日本のディズニーパーク。\" +\\\n",
        "\"年間来場者数は日本最大の約1,600万人で、世界のテーマパーク・アミューズメントパークの中でも、\" +\\\n",
        "\"フロリダ州のウォルト・ディズニー・ワールド・リゾートのマジック・キングダム、カリフォルニア州の\" +\\\n",
        "\"ディズニーランド・リゾートのディズニーランド・パークに次いで世界3位の規模を誇る[1]。オリエンタルランド\" +\\\n",
        "\"がザ・ウォルト・ディズニー・カンパニーとのライセンス契約のもと運営している[3]。\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I02IfWSd-CdX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "extractor = pke.unsupervised.MultipartiteRank()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsO6EL6U-E_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "extractor.load_document(input=text, language='ja_ginza', normalization=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOGsxBFq-Hf3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "extractor.candidate_selection(pos={'NOUN', 'PROPN', 'ADJ', 'NUM'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Es1xKQ--Jhg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "extractor.candidate_weighting(threshold=0.74, method='average', alpha=1.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_1Xk2j_-Ltg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "0aafc07e-a930-4698-b65a-8dc977f52236"
      },
      "source": [
        "extractor.get_n_best(n=10)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('東京 ディズニーランド', 0.1267660973254461),\n",
              " ('ディズニー リゾート', 0.09318284258415595),\n",
              " ('ディズニー パーク', 0.0812808350466151),\n",
              " ('テーマ パーク', 0.0620427285940315),\n",
              " ('千葉県浦安市舞浜', 0.05491005436297722),\n",
              " ('tokyo disneyland', 0.0537149605504754),\n",
              " ('tdl', 0.050535126657940015),\n",
              " ('ウォルト', 0.04898900391347567),\n",
              " ('リゾート', 0.04835558989897219),\n",
              " ('ディズニー', 0.04647126812366527)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvYvryAM7Rjl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('ja_ginza')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HIaruFP7ptN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pke"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpq4qVSz7uKq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pke.base.ISO_to_language['ja_ginza'] = 'japanese'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjHp7if67S1p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "extractor = pke.unsupervised.MultipartiteRank()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJstuzlN8A0u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pke_koji(text):\n",
        "  print(text)\n",
        "  try :\n",
        "    extractor = pke.unsupervised.MultipartiteRank()\n",
        "    extractor.load_document(input=text, language='ja_ginza', normalization=None)\n",
        "    extractor.candidate_selection(pos={'NOUN', 'PROPN', 'ADJ', 'NUM'})\n",
        "    extractor.candidate_weighting(threshold=0.8, method='average', alpha=1.1)\n",
        "    result = extractor.get_n_best(n=10)\n",
        "    m = [i[0] for i in result]\n",
        "    text = ';'.join(m)\n",
        "    print(text)\n",
        "    return text      \n",
        "  except Exception as e:\n",
        "      print(e)\n",
        "      return ''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HU_o5-Z8CHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LdiGS3_7FfC",
        "colab_type": "code",
        "outputId": "45205339-1c67-40d5-ae57-01798deda64f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd sentence-transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/sentence-transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzwaFQcF7HCf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "model_path = \"/content/training_bert_japanese\"\n",
        "model = SentenceTransformer(model_path, show_progress_bar=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcSVgPsh7K9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIt15akVChOn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "192b1b3e-7a03-48d9-88cd-9072d52a57fd"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVNt4A8b8sjU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "030cf2be-59d5-4602-f3de-67d22a009e8d"
      },
      "source": [
        "%cd sentence-transformers/"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/sentence-transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh8nAv7EHoWW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('merge_data.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01s1ls5TCz74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.dropna(subset  = ['text','name'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UeIdztzIkP-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "fa86c8b3-c4ed-41ea-8a63-b219a29c98a7"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": 301,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'name', 'url', 'ingredient', 'step', 'description',\n",
              "       'text'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 301
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChZolfcRHyIb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['step_g'] = df['step'] + ' ' + df['ingredient'] # + ' ' + df['step']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pafh6FA8Bjd3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['description'] = df['name'] + df['text']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEPQiNkgMMFY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.dropna(subset=['name'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq_4qM0llVbG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df[~df.duplicated(subset='url')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dqyike2XloZg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "be3e42ec-1d8c-4d84-ec4f-faf2c57152fc"
      },
      "source": [
        "df[df['description'].isnull()]"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>name</th>\n",
              "      <th>url</th>\n",
              "      <th>ingredient</th>\n",
              "      <th>step</th>\n",
              "      <th>description</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Unnamed: 0, name, url, ingredient, step, description, text]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cJFZZVB9P2a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['step_ext'] =  df['step'].apply( pke_koji )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVGqUW7VXCy0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['ingredient_ext'] =  df['ingredient'].apply( pke_koji )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07Y776Y82E65",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['description'] = df['name']   + ' ' + df['text'] + ' ' + df['ingredient_ext'] +  ' ' + df['step_ext']  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZCIJW5tZXLg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=df[df['description'].str.len() > 100]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfrCgcps7br3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = df['description'].values.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUmCZ-Gx7g2X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_vectors = model.encode(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyhY3-AwNECd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_vector = pd.Series(sentence_vectors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLtBftAN8vV7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences_name = df['name'].values.tolist()\n",
        "sentences_url = df['url'].values.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwfKoVBh7yYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "num_clusters = 8\n",
        "clustering_model = KMeans(n_clusters=num_clusters)\n",
        "clustering_model.fit(sentence_vectors)\n",
        "cluster_assignment = clustering_model.labels_\n",
        "\n",
        "clustered_sentences = [[] for i in range(num_clusters)]\n",
        "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
        "    clustered_sentences[cluster_id].append(sentences_name[sentence_id][:10])\n",
        "\n",
        "for i, cluster in enumerate(clustered_sentences):\n",
        "    print(\"Cluster \", i+1)\n",
        "    print(cluster)\n",
        "    print(\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P333vaBd6Kzf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_queries = pd.read_csv('kurashiru_keyword.txt',header = None)\n",
        "#df_queries = pd.read_csv('healsio_recipe.csv',header = None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcW32A09wjFm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences_names = df['name'].values.tolist()\n",
        "sentences_urls = df['url'].values.tolist()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIJoqeDa8KLK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data1 = np.ndarray([])\n",
        "import scipy.spatial\n",
        "\n",
        "#queries = ['ゆきぽよ','美術館', 'グルメ', '究極の料理','スポーツ大会', '子供がはしゃげる','東広島市　料理','東広島市　おでかけ','古墳','アニメ','細菌','ビッグバン','甲賀忍者']\n",
        "queries = df_queries[0].values.tolist()\n",
        "query_embeddings = model.encode(queries)\n",
        "\n",
        "closest_n = 5\n",
        "#df['data'] = ''\n",
        "for i,(name,url,query, query_embedding) in enumerate(zip(sentences_names,sentences_urls,queries, query_embeddings)):\n",
        "    distances = scipy.spatial.distance.cdist([query_embedding], sentence_vectors, metric=\"cosine\")[0]\n",
        "\n",
        "    results = zip(range(len(distances)), distances)\n",
        "    results = sorted(results, key=lambda x: x[1])\n",
        "\n",
        "    print(\"\\n\\n======================\\n\\n\")\n",
        "    print(\"Query:\", query + ' ' )\n",
        "    print(\"\\nTop 5 most similar sentences in corpus:\")\n",
        "\n",
        "    for j,(idx, distance) in enumerate(results[0:closest_n]):\n",
        "#        df_queries.loc[i,'data'] += ' ' +sentences_name[idx].strip() + ' '  + sentences_url[idx].strip()\n",
        "        print(sentences_names[idx].strip(),sentences_urls[idx].strip(),\"(Score: %.4f)\" % (distance / 2),idx)\n",
        "        if j == 0 and i == 0:\n",
        "          data1 = sentence_vectors[idx]\n",
        "          print(idx)\n",
        "        elif j == 0:\n",
        "          data1 = np.vstack((data1, sentence_vectors[idx] ) )\n",
        "          print(idx)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YC9LJM5XGuCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmynqaXqFMcY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv('キーワード抽出(手順　材料別々)　100文字数以上　制限2.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGpFB8rrGqsv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "3a1824a6-dcca-453c-eb93-cd8f8f033553"
      },
      "source": [
        "pip install rank_bm25"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rank_bm25\n",
            "  Downloading https://files.pythonhosted.org/packages/d2/e4/38d03d6d5e2deae8d2838b81d6ba2742475ced42045f5c46aeb00c5fb79c/rank_bm25-0.2.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from rank_bm25) (1.18.4)\n",
            "Building wheels for collected packages: rank-bm25\n",
            "  Building wheel for rank-bm25 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rank-bm25: filename=rank_bm25-0.2-cp36-none-any.whl size=4162 sha256=111b2d8641719a22bf0d909fa9e658dc8cd95ebdc79fedd59845b296bccdfa78\n",
            "  Stored in directory: /root/.cache/pip/wheels/6f/0c/1f/78945dd6a5478bbcdb50d73ac96ae5af2ffcdfcd374fd9b1bf\n",
            "Successfully built rank-bm25\n",
            "Installing collected packages: rank-bm25\n",
            "Successfully installed rank-bm25-0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PH8pXM32Gu7u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "corpus = [\n",
        "    \"私 は　明智　光秀　です!\",\n",
        "    \"今日 曇り\",\n",
        "    \"How is the weather today?\"\n",
        "]\n",
        "\n",
        "tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
        "\n",
        "bm25 = BM25Okapi(tokenized_corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egiSh3dyGw-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query = \"私　は　 信長\"\n",
        "tokenized_query = query.split(\" \")\n",
        "\n",
        "doc_scores = bm25.get_scores(tokenized_query)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aet7viqgGz9S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5c07052f-7feb-48ef-d0be-ea77f97224a0"
      },
      "source": [
        "doc_scores"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wplK8Z-OG3RR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "44aea061-c13c-439f-877c-ef871b84a373"
      },
      "source": [
        "bm25.get_top_n(tokenized_query, corpus, n=3)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['How is the weather today?', '今日 曇り', '私 は\\u3000明智\\u3000光秀\\u3000です!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8EgKdfeG768",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cf70a4d8-caec-4416-fe0a-f4e02a92ae29"
      },
      "source": [
        "pip install mecab-python3"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mecab-python3 in /usr/local/lib/python3.6/dist-packages (0.996.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXMnMn8rHARB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import MeCab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sT3Yi80sHCB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9IMvC_CHER4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3a54aef0-013f-4014-bc7e-0c3c6a172537"
      },
      "source": [
        "tokenizer =  MeCab.Tagger(\"-Owakati\")  \n",
        "sentences = []\n",
        "print (\"Parsing sentences from training set...\")\n",
        "\n",
        "\n",
        "# Loop over each news article.\n",
        "for review in df['description']:\n",
        "    #print(review)\n",
        "    try:\n",
        "        # Split a review into parsed sentences.\n",
        "        result = tokenizer.parse(review).replace(\"\\u3000\",\"\").replace(\"\\n\",\"\")\n",
        "        result = re.sub(r'[0123456789０１２３４５６７８９！＠＃＄％＾＆\\-|\\\\＊\\“（）＿■×※⇒—●(：〜＋=)／*&^%$#@!~`){}…\\[\\]\\\"\\'\\”:;<>?＜＞？、。・,./『』【】「」→←○]+　', \"\", result)\n",
        "       # h = result.split(\" \")\n",
        "      #  h = list(filter((\"\").__ne__, h))\n",
        "       # print(h)\n",
        "        sentences.append(result)\n",
        "    except:\n",
        "        continue"
      ],
      "execution_count": 322,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parsing sentences from training set...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHvYiVuBHenP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ac8ee814-e5d8-479e-eb71-c6b06414641f"
      },
      "source": [
        "names = []\n",
        "print (\"Parsing sentences from training set...\")\n",
        "\n",
        "\n",
        "# Loop over each news article.\n",
        "for review in df['name']:\n",
        "    #print(review)\n",
        "    try:\n",
        "        # Split a review into parsed sentences.\n",
        "        result = tokenizer.parse(review).replace(\"\\u3000\",\"\").replace(\"\\n\",\"\")\n",
        "        result = re.sub(r'[0123456789０１２３４５６７８９！＠＃＄％＾＆\\-|\\\\＊\\“（）＿■×※⇒—●(：〜＋=)／*&^%$#@!~`){}…\\[\\]\\\"\\'\\”:;<>?＜＞？、。・,./『』【】「」→←○]+　', \"\", result)\n",
        "       # h = result.split(\" \")\n",
        "      #  h = list(filter((\"\").__ne__, h))\n",
        "       # print(h)\n",
        "        names.append(result)\n",
        "    except:\n",
        "        continue"
      ],
      "execution_count": 323,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parsing sentences from training set...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSIpIiNxHG78",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "tokenized_corpus = [doc.split(\" \") for doc in sentences]\n",
        "\n",
        "bm25_test = BM25Okapi(tokenized_corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUal8YN5Hqg6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = '簡単 '"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYkkocCFHKva",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "query = result\n",
        "tokenized_query = query.split(\" \")\n",
        "\n",
        "doc_scores = bm25_test.get_scores(tokenized_query)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hF5_VX1THPAg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bm25_test.get_top_n(tokenized_query, names, n=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXBcs7bstI_f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "f7cac09c-3ac8-4a67-9474-cdc75ceb8bfd"
      },
      "source": [
        "df['explain'] = df['name'] + ' ' + df['step'] + ' ' + df['ingredient']"
      ],
      "execution_count": 329,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmEjA1xUtdJQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c856aaee-3eaa-410a-c114-c91224d353c3"
      },
      "source": [
        "tokenizer =  MeCab.Tagger(\"-Owakati\")  \n",
        "sentences = []\n",
        "print (\"Parsing sentences from training set...\")\n",
        "\n",
        "\n",
        "# Loop over each news article.\n",
        "for review in df['explain']:\n",
        "    #print(review)\n",
        "    try:\n",
        "        # Split a review into parsed sentences.\n",
        "        result = tokenizer.parse(review).replace(\"\\u3000\",\"\").replace(\"\\n\",\"\")\n",
        "        result = re.sub(r'[0123456789０１２３４５６７８９！＠＃＄％＾＆\\-|\\\\＊\\“（）＿■×※⇒—●(：〜＋=)／*&^%$#@!~`){}…\\[\\]\\\"\\'\\”:;<>?＜＞？、。・,./『』【】「」→←○]+　', \"\", result)\n",
        "       # h = result.split(\" \")\n",
        "      #  h = list(filter((\"\").__ne__, h))\n",
        "       # print(h)\n",
        "        sentences.append(result)\n",
        "    except:\n",
        "        continue"
      ],
      "execution_count": 330,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parsing sentences from training set...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7Q0S9sPtY5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "tokenized_corpus = [doc.split(\" \") for doc in sentences]\n",
        "\n",
        "bm25_test = BM25Okapi(tokenized_corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyyNSdlXgjDb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_queries = pd.read_csv('kurashiru_keyword.txt',header = None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bdmNdSrf8QR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#queries = ['ゆきぽよ','美術館', 'グルメ', '究極の料理','スポーツ大会', '子供がはしゃげる','東広島市　料理','東広島市　おでかけ','古墳','アニメ','細菌','ビッグバン','甲賀忍者']\n",
        "queries = df_queries[0].values.tolist()\n",
        "query_embeddings = model.encode(queries)\n",
        "\n",
        "closest_n = 5\n",
        "#df['data'] = ''\n",
        "for i,(name,url,query, query_embedding) in enumerate(zip(sentences_names,sentences_urls,queries, query_embeddings)):\n",
        "\n",
        "    tokenized_query = query.split(\" \")\n",
        "    print(\"\\n\\n======================\\n\\n\")\n",
        "    print(\"Query:\", query + ' ' )\n",
        "    print(\"\\nTop 5 most similar sentences in corpus:\")\n",
        "\n",
        "    doc_scores = bm25_test.get_scores(tokenized_query)\n",
        "    print(bm25_test.get_top_n(tokenized_query, names, n=5))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJltV8Z_skU2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "f2cd96ec-72bc-45ef-c835-6ecccdf535e8"
      },
      "source": [
        "df_h = pd.read_csv('healsio_recipe.csv')"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-e8abd55b3cb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'healsio_recipe.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File healsio_recipe.csv does not exist: 'healsio_recipe.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gu3u5UEQEJ4l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_h = pd.read_csv('merge_data.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nntWdupxtrym",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_h['description'] = df_h['name'] #+ ' ' + df_h['text'] #  +  df['step']#+ df['ingredient'] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXmaG4G3uF2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_h = df_h[~df_h.duplicated(subset='url')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IKMSgGKETeJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_h = df_h.replace('セットメニュー用レシピ','')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93q1QGXKEZIS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_h = df_h.dropna( subset = ['text'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYMi4Z0ZFIDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_h = df_h.dropna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cT5CrdvtuI_o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences =  df_h['description'].values.tolist()\n",
        "sentences_name = df_h['name'].values.tolist()\n",
        "sentences_url = df_h['url'].values.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PD7D-hertQnX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_vectors = model.encode(sentences)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnMWbiBztSYe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df_queries = pd.read_csv('cookpad_keyword.txt',header = None)\n",
        "df_queries = pd.read_csv('kurashiru_keyword.txt',header = None)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ark-rDSVwvYF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_out = pd.read_csv('output.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvxEV-HntThd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy.spatial\n",
        "\n",
        "queries = ['ゆきぽよ','美術館', 'グルメ', '究極の料理','スポーツ大会', '子供がはしゃげる','東広島市　料理','東広島市　おでかけ','古墳','アニメ','細菌','ビッグバン','甲賀忍者']\n",
        "queries = df_queries[0].values.tolist()\n",
        "query_embeddings = model.encode(queries)\n",
        "\n",
        "closest_n = 5\n",
        "adata = pd.DataFrame()\n",
        "for query, query_embedding in zip(queries, query_embeddings):\n",
        "    distances = scipy.spatial.distance.cdist([query_embedding], data1, metric=\"cosine\")[0]\n",
        "\n",
        "    results = zip(range(len(distances)), distances)\n",
        "    results = sorted(results, key=lambda x: x[1])\n",
        "\n",
        "    print(\"\\n\\n======================\\n\\n\")\n",
        "    print(\"Query:\", query)\n",
        "    print(\"\\nTop 5 most similar sentences in corpus:\")\n",
        "    i = 0\n",
        "    for idx, distance in results[0:closest_n]:\n",
        "        i+=1\n",
        "        print(sentences_name[idx].strip(), sentences_url[idx].strip(),\"(Score: %.4f)\" % (distance / 2))\n",
        "        temp = pd.DataFrame({'query': query,\n",
        "                   'name':sentences_name[idx].strip(),'rank': i,\n",
        "                   'url': sentences_url})\n",
        "        adata = pd.concat([adata,temp], axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VG5Ty2PA_TZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "import os\n",
        "logs_base_dir = \"runs\"\n",
        "os.makedirs(logs_base_dir, exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbPTJ-nnBA3A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorboard as tb\n",
        "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n",
        "\n",
        "summary_writer = SummaryWriter()\n",
        "summary_writer.add_embedding(mat=np.array(sentence_vectors), metadata=sentences_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydYLaWoKBCOI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir {logs_base_dir}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhgZhlUoGI1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kill 1265"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}